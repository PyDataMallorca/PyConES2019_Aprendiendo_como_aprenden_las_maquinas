{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer clasificación lineal podemos usar algoritmos diferentes a los vistos anteriormente. Entre estos algoritmos encontramos la regresión logística. A pesar del nombre, NO sirve para hacer regresión y solo se usa para clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente se usa para hacer clasificación binaria aunque se puede extender (la regresión logística y otros algoritmos de clasificación binaria) a clasificación multiclase como luego veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este caso es similar a lo que vimos anteriormente, tenemos lo siguiente:\n",
    "\n",
    "$$ \\hat{y}(x) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "Aquí $ w_0 = b $ y $ x_0 = 1 $.\n",
    "\n",
    "Y nuestras clases las podemos definir como:\n",
    "\n",
    "$$ clase_{1} = 1 \\space si \\space \\hat{y}(x)\\ge 0.5 $$\n",
    "$$ clase_{0} = 0 \\space si \\space \\hat{y}(x)\\lt 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las clases da igual como las definamos, pueden ser \"Sí\" y \"No\" o \"No\" y \"Sí\", es decir, simplemente tiene que ser coherente con la pregunta.\n",
    "\n",
    "¿Es azul? Sí(+1)/No(-1).\n",
    "\n",
    "¿Es azul? Sí(-1)/No(+1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, ¿cuál sería una buena función de pérdida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Usamos el error cuadrático medio como en el caso de la regresión lineal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear to logistic](./imgs/13_linear_vs_logistic_regression.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que queremos son probabilidades que nos digan si algo es más probable que esté en una clase o en otra. Para ello se usa la función logística o sigmoide (por su forma en ese):\n",
    "\n",
    "$$ S(y_i) = \\frac{1}{1+e^{-y_i}} $$\n",
    "\n",
    "Pero, ¿cómo llegamos a eso?\n",
    "\n",
    "La probabilidad de que sea 1 debido a sus entradas se puede representar de la siguiente forma:\n",
    "\n",
    "$$ P(y=1|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si pensamos que eso se puede representar de forma lineal y tenemos n dimensiones tendríamos algo como:\n",
    "\n",
    "$$ P(y=1|x) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "($ w_0 = b $ y $ x_0 = 1 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de regresión podría ajustar esos pesos pero, como hemos visto en la gráfica de más arriba eso podría llevar a valores que van de $-\\infty$ a $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ratio de probabilidades (*odds ratio*) es un término que nos puede ayudar. Se define como la probabilidad de que suceda entre la probabilidad de que no suceda. Como nos encontramos en un caso binario, la probabilidad de que suceda es $p$ y la probabilidad de que no suceda es el resto hasta 1, $1-p$. Su ratio se representa así:\n",
    "\n",
    "$$ odds(p) = \\frac{p}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si representamos lo anterior para diferentes valores de $p$ tenemos la siguiente gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer, make_blobs\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score,\n",
    "                             roc_curve, roc_auc_score)\n",
    "\n",
    "from lib.datasets import make_forge\n",
    "from lib.plots import plot_2d_separator, plot_2d_classification\n",
    "from lib import discrete_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01)\n",
    "odds = x / (1 - x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\frac{p}{1-p}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $p=1$ nuestro ratio se dispara lo cual no es lo que queremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el logaritmo natural del ratio y nos da lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.01, 1, 0.01)\n",
    "odds = np.log(x / (1 - x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\log(\\frac{p}{1-p})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos mapear una combinación lineal de entradas arbitrarias a la función del logaritmo natural de los ratios podemos aprovechar eso para tener:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "($ w_0 = b $ y $ x_0 = 1 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos la $ P(y=1|x) $ podemos buscar la inversa del logaritmo natural ($log\\_odds$) para obtenerlo. Si nos aprovechamos de algunas identidades de logaritmos y exponenciales:\n",
    "\n",
    "$$ y = log(\\frac{x}{1-x}) $$\n",
    "\n",
    "$$ x = log(\\frac{y}{1-y}) $$\n",
    "\n",
    "$$ e^x = \\frac{y}{1-y} $$\n",
    "\n",
    "$$y = (1-y)*e^x $$\n",
    "\n",
    "$$ y = e^x - y*e^x $$\n",
    "\n",
    "$$ y + ye^x = e^x $$\n",
    "\n",
    "$$ y*(1 + e^x) = e^x $$\n",
    "\n",
    "$$ y = \\frac{e^x}{1+e^x} $$\n",
    "\n",
    "$$ y = \\frac{1}{\\frac{1}{e^x} + 1} $$\n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última expresión es la inversa de $lod\\_odds$ y se parece mucho a la función logística o sigmoide (en realidad lo es...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.01)\n",
    "inv_odds = 1 / (1 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, inv_odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\frac{1}{1+e^{-x}}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, ahora todo se restringe a 0 y 1 en el eje *y*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, volviendo al principio teníamos que:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "($ w_0 = b $ y $ x_0 = 1 $)\n",
    "\n",
    "Si simplificamos la notación tenemos:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w^T·x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora, finalmente, sacamos la inversa de lo anterior nos quedará:\n",
    "\n",
    "$$ P(y=1|x) = \\frac{1}{1+e^{-w^T·x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores de máxima verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inglés se conocen como *maximum likelihood estimators* (MLE). Es un método para obtener los parámetros de un modelo estadístico. El método obtiene los parámetros buscando los valores de los parámetros que maximizan la función de verosimilitud. Las estimaciones se conocen como estimadores de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como funciona esto en la práctica. Para ello vamos a usar la distribución normal:\n",
    "\n",
    "$$ f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp(\\frac{-(x-\\mu)^2}{2\\sigma^2})  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de verosimilitud se obtiene de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\mu, \\sigma^2) = f(x_1, x:2,...,x_N|\\mu, \\sigma^2) = \\prod_{i=1}^N f(x_i|\\mu, \\sigma^2) $$\n",
    "$$ L(\\mu, \\sigma^2) = (\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que queremos hacer es maximizar lo anterior. En la expresión anterior tenemos multiplicaciones pero lo que se suele hacer normalmente es usar el logaritmo de la función de verosimilitud. Y en lugar de maximizar se minimiza usando el negativo del logaritmo de la función de verosimilitud:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ll(\\mu, \\sigma^2) = \\log(L(\\mu, \\sigma^2)) $$\n",
    "\n",
    "$$ ll(\\mu, \\sigma^2) = \\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log(L(\\mu, \\sigma^2)) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[INCISO]** Un breve paréntesis para anotar un par de propiedades de exponenciales y de logaritmos naturales:\n",
    "\n",
    "$$ e^x · e ^y = e^{(x + y)} $$\n",
    "\n",
    "$$ ln(x·y) = ln(x) + ln(y) $$\n",
    "\n",
    "$$ ln(x/y) = ln(x) - ln(y) $$\n",
    "\n",
    "$$ ln(x^y) = y·ln(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con MLE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desarrollamos la expresión anterior de $nll$ usando las propiedades de exponenciales y logaritmos nos queda:\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)}) - (\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2}) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\frac{n}{2}\\log(\\frac{1}{2\\pi\\sigma^2}) - (\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para minimizar hemos de usar el gradiente e igualarlo a 0. Si derivamos con respecto a $\\mu$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\mu}} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\mu}} = \\frac{\\sum_{i=0}^{N}\\mu -\\sum_{i=0}^{N}x_i}{2\\sigma^2}  = 0 $$\n",
    "\n",
    "$$ \\sum_{i=0}^{N}\\mu -\\sum_{i=0}^{N}x_i = 0 $$\n",
    "\n",
    "$$ n·\\mu -\\sum_{i=0}^{N}x_i = 0 $$\n",
    "\n",
    "$$ \\mu = \\frac{\\sum_{i=0}^{N}x_i}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma, derivamos $nll$ con respecto a $\\sigma$ e igualamos a 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\sigma}} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\sigma}} = \\frac{n}{\\sigma} - \\frac{\\sum_{i=1}^{N}(x_i-\\mu)^2}{\\sigma^3}= 0 $$\n",
    "\n",
    "$$ \\sigma^2 = \\sum_{i=1}^{N}\\frac{(x_i-\\mu)^2}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma obtendríamos los parámetros de la distribución normal.\n",
    "\n",
    "Vamos a aplicar lo mismo ahora para el caso de la regresión logística:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de verosimilitud será (para una clasificación binaria) para un caso (un ejemplo $(x_i, y_i)$) será:\n",
    "\n",
    "$$ L(\\textbf{w})_i = p(x_i|\\textbf{w})^{y_i} · (1-p(x_i|\\textbf{w})^{1-y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo generalizamos para los n casos (datos) tenemos la función de verosimilitud:\n",
    "\n",
    "$$ L(\\textbf{w}) = \\prod_{i=1}^{N}p(x_i|\\textbf{w})^{y_i} · (1-p(x_i|\\textbf{w})^{1-y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *log-likelihood* será:\n",
    "\n",
    "$$ ll(\\textbf{w}) = \\sum_{i=1}^{N}({y_i}·\\log(p(x_i|\\textbf{w})) + {1-y_i}·\\log(1-p(x_i|\\textbf{w}))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si toqueteamos lo anterior llegaremos a lo siguiente (saltando pasos):\n",
    "\n",
    "$$ ll = \\sum_{i=1}^{N}y_{i}\\textbf{w}^{T}x_{i} - log(1+e^{\\textbf{w}^{T}x_{i}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si de lo anterior sacamos el gradiente (saltamos pasos):\n",
    "\n",
    "$$ \\bigtriangledown ll = X^{T}(Y - \\hat{Y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente del *log-likelihood* no es más que la multiplicación de la traspuesta de las entradas por el error de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de la regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con toda esta chicha vamos a construir nuestro algoritmo de regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegLog:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, num_iters=50_000):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.lr = learning_rate\n",
    "        self.ni = num_iters\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # weights initialization\n",
    "        intercept = np.ones((x.shape[0], 1))\n",
    "        self.X = np.concatenate((intercept, x), axis=1)\n",
    "        self.y = y\n",
    "        self.w = np.zeros(self.X.shape[1])\n",
    "        \n",
    "        for i in range(self.ni):\n",
    "            z = np.dot(self.X, self.w)\n",
    "            h = self._sigmoid(z)\n",
    "            gradient = np.dot(self.X.T, (h - y)) / y.size\n",
    "            self.w -= self.lr * gradient\n",
    "            \n",
    "            z = np.dot(self.X, self.w)\n",
    "            h = self._sigmoid(z)\n",
    "            loss = self._loss(h, self.y)\n",
    "                \n",
    "    def predict_prob(self, x):\n",
    "        intercept = np.ones((x.shape[0], 1))\n",
    "        x = np.concatenate((intercept, x), axis=1)\n",
    "        return self._sigmoid(np.dot(x, self.w))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.predict_prob(x).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver qué tal funciona. Obtenemos unos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_forge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pintamos para ver cómo son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "idx = y_train == 0\n",
    "ax.scatter(X_train[idx, 0], X_train[idx, 1], color='b', label=\"clase 0\")\n",
    "ax.scatter(X_train[~idx, 0], X_train[~idx, 1], color='r', label=\"clase 1\")\n",
    "ax.legend()\n",
    "idx = y_test == 0\n",
    "ax.scatter(X_test[idx, 0], X_test[idx, 1], color='b', s=200)\n",
    "ax.scatter(X_test[~idx, 0], X_test[~idx, 1], color='r', s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestra clase recien creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog = RegLog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos con nuestro modelo ajustado los valores de prueba para ver si se parecen a las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reglog.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yi, yyi, x0, x1 in zip(y_pred, y_test, X_test[:,0], X_test[:,1]):\n",
    "    print(int(yi), yyi, x0, x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comparar con lo que trae `scikit-learn`. Instanciamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e20, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yi, yyi in zip(y_pred, y_test):\n",
    "    print(int(yi), yyi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que lo que obtenemos es similar a nuestro algoritmo de más arriba. Vamos a ver los pesos que obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reglog.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.intercept_, logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parecen bastante. Podéis modificar la tasa de aprendizaje o el número de iteraciones para ver si mejora o empeora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más detalles y cosas interesantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la instancia de más arriba de `LogisticRegression` estoy usando un parámetro que le llama `C`. Este hiperparámetro es el que regula la regularización. Por defecto, la regresión logística en `scikit-learn` usa regularización L2 y valores más altos de `C` llevan a quitarle importancia a la regularización (al revés que con el $\\alpha$ de la regressión *Ridge* o la regresión Lasso). Por eso, en el ejemplo de más arriba he usado `C=1e20` para que la regularización fuese poco importante ya que en nuestro algoritmo no hemos incluido regularización para simplificar las cosas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el modelo que implementa `scikit-learn` con los valores por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varias cosas de las que se ven ahí arriba. \n",
    "\n",
    "* El *solver* es lo que realiza la optimización. Podéis leer más sobre ello aquí: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html. En el caso de scikit-learn usa *solvers* especializados. Más info aquí: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "* `C` es el parámetro que regula la regularización. Un valor bajo regulariza mucho mientras que un valor muy alto le quita importancia a la regularización.\n",
    "\n",
    "* `class_weight` nos permite dar más peso a alguna clase (acordáos de los datos desnivelados o descompensados (*imbalanced datasets*).\n",
    "\n",
    "* `penalty` define el tipo de regularización. Dependiendo del *solver* se podrán usar unos tipos de regularizaciones u otros.\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veis, hay muchas cosas que se pueden toquetear y que llevarán a resultados diferentes. Dependiendo de los datos que tengamos tendrá sentido usar unas cosas u otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como afecta el parámetro `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i, C in enumerate([0.001, 0.01, 1, 100, 10000]):\n",
    "    logreg = LogisticRegression(C=C).fit(X, y)\n",
    "    plot_2d_separator(logreg, X, fill=False, eps=0.5, ax=axs[i], alpha=.7)\n",
    "    discrete_scatter(X[:, 0], X[:, 1], y, ax=axs[i])\n",
    "    axs[i].set_title(f\"C={C}\")\n",
    "    axs[i].set_xlabel(\"Clase 0\")\n",
    "    axs[i].set_ylabel(\"Clase 1\")\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por mucho que nos empeñemos, en el conjunto de datos anterior con una separación lineal es imposible tenerlo todo completamente separado y puede parecer muy limitado. Cuando aumenta el número de dimensiones es cuando estos modelos pueden adquirir más importancia. Vamos a usar los datos *Cancer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, \n",
    "    cancer.target, \n",
    "    stratify=cancer.target, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo anterior usamos la *keyword* `stratify` para balancear la misma proporción de etiquetas en los datos de entrenamiento y de prueba, es decir que tengan la misma proporción de etiquetas en ambos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.sum() / len(y_train), y_test.sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el modelo y mostramos qué tal se comporta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(logreg.score(X_train, y_train))\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que el algoritmo va muy bien. El hecho de que el *score* en ambos casos esté muy cerca y en la prueba esté dando levemente mejor que en el entrenamiento quizá sea indicativo de que estamos infraestimando. Vamos a tocar el valor de `C` para que regularice menos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(logreg100.score(X_train, y_train))\n",
    "print(logreg100.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso conseguimos subir el valor de *score* y en el entrenamiento es levemente superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos lo mismo pero ahora regularizando más que en el valor por defecto veamos a ver qué sale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
    "print(logreg001.score(X_train, y_train))\n",
    "print(logreg001.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso los valores son más bajos. Tenemos un modelo demasiado simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.plot(logreg.coef_.T, 'ko', label=\"C=1\")\n",
    "ax.plot(logreg100.coef_.T, 'b^', label=\"C=100\")\n",
    "ax.plot(logreg001.coef_.T, 'yv', label=\"C=0.001\")\n",
    "ax.set_xticks(range(cancer.data.shape[1]))\n",
    "ax.set_xticklabels(cancer.feature_names, rotation=90)\n",
    "ax.hlines(0, 0, cancer.data.shape[1])\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlabel(\"Coeficiente\")\n",
    "ax.set_ylabel(\"Magnitud del coeficiente\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos un modelo más interpretable, como vimos anteriormente, quizá nos interese usar una regularización L1 ya que vemos que muchas dimensiones están cercanas a 0 independientemente de la magnitud de la regularización L2 que usemos. Esto nos puede estar indicando que quizá sea conveniente deshacerse de algo de \"ruido\".\n",
    "\n",
    "Vamos a hacer lo mismo que antes pero con regularización L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "for C, marker_set in zip([0.001, 1, 100], ['ko', 'b^', 'yv']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
    "    print(\"Train\", C, lr_l1.score(X_train, y_train))\n",
    "    print(\"Test \", C, lr_l1.score(X_test, y_test))\n",
    "    ax.plot(lr_l1.coef_.T, marker_set, label=f\"C={C:.3f}\")\n",
    "    ax.set_xticks(range(cancer.data.shape[1]))\n",
    "    ax.set_xticklabels(cancer.feature_names, rotation=90)\n",
    "    ax.hlines(0, 0, cancer.data.shape[1])\n",
    "    ax.set_xlabel(\"Coeficiente\")\n",
    "    ax.set_ylabel(\"Magnitud del coeficiente\")\n",
    "    ax.set_ylim(-5, 5)\n",
    "ax.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, los algoritmos de clasificación lineales guardan muchas similitudes con los algoritmos de regresión lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos hablado de la matriz de confusión. Vamos a ver como se ven los modelos usando la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg01 = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "logreg10 = LogisticRegression(C=10).fit(X_train, y_train)\n",
    "\n",
    "y_pred01 = logreg01.predict(X_test)\n",
    "y_pred10 = logreg10.predict(X_test)\n",
    "\n",
    "mc01 = confusion_matrix(y_test, y_pred01)\n",
    "mc10 = confusion_matrix(y_test, y_pred10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mc01)\n",
    "print(mc10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.target[0], cancer.target_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, la clase 0 se asigna a los cánceres malignos. Sabiendo esto vamos a dibujar la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "axs[0].set_xticks([0, 1])\n",
    "axs[0].set_yticks([0, 1])\n",
    "sns.heatmap(pd.DataFrame(mc01), annot=True, cmap=\"YlGnBu\" ,fmt='g', ax=axs[0])\n",
    "axs[0].xaxis.set_label_position(\"top\")\n",
    "axs[0].set_title('Matriz de Confusión (C=0.01)', y=1.1)\n",
    "axs[0].set_ylabel('Actual label')\n",
    "axs[0].set_xlabel('Predicted label')\n",
    "\n",
    "axs[1].set_xticks([0, 1])\n",
    "axs[1].set_yticks([0, 1])\n",
    "sns.heatmap(pd.DataFrame(mc10), annot=True, cmap=\"YlGnBu\" ,fmt='g', ax=axs[1])\n",
    "axs[1].xaxis.set_label_position(\"top\")\n",
    "axs[1].set_title('Matriz de Confusión (C=10)', y=1.1)\n",
    "axs[1].set_ylabel('Actual label')\n",
    "axs[1].set_xlabel('Predicted label')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior hemos conseguido, regularizando menos, reducir el número de falsos negativos. En casos como estos, donde un falso positivo requeriría hacer más pruebas y luego descartar el problema, un falso negativo es un problema mucho más grave ya que se puede mandar a una persona a casa con un cáncer maligno habiéndole dicho que está todo correcto. En estos casos hemos de ver qué es lo que nos interesa optimizar mejor. Igual nos da un poco más igual tener falsos positivos pero es inaceptable tener falsos negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar más métricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Caso C=0.01\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred01))\n",
    "print(\"Precision:\",precision_score(y_test, y_pred01))\n",
    "print(\"Recall:\",recall_score(y_test, y_pred01))\n",
    "\n",
    "print(\"Caso C=10\")\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred10))\n",
    "print(\"Precision:\",precision_score(y_test, y_pred10))\n",
    "print(\"Recall:\",recall_score(y_test, y_pred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior nos interesaría que nuestras métricas optimizasen el caso de evitar mandar a gente enferma a casa ya que le dejar a sanos en el hospital puede ser un incordio más aceptable.\n",
    "\n",
    "Existen más métricas interesantes. Podéis mirar en:\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algoritmos de clasificación que no permiten la clasificación multiclase. La regresión logística es una excepción. Una técnica común para poder usar un algoritmo que solo permite clasificación binaria a un algoritmo con capacidad de clasificación multiclase es usar la estrategia \"Uno contra el Resto\" (*One-Vs-Rest*). La estrategia \"Uno contra el Resto\" no es más que etiquetar la clase que queremos clasificar (por ejemplo con un 1) contra el resto de clases (todas tendrán clase, por ejemplo, 0). De tal forma que tendremos tantos modelos de clasificación binaria como clases queramos clasificar. Para hacer una predicción todos los clasificadores binarios se ejecutan usando un punto de prueba. El clasificador que tiene el *score* más alto en su clase será el \"ganador\" y esa clase se devuelve como el resultado de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver esto con un ejemplo para ver si se entiende mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dibujar estos datos a ver cómo se ven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "ax.set_xlabel(\"Feature 0\")\n",
    "ax.set_ylabel(\"Feature 1\")\n",
    "ax.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos una regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(multi_class=\"ovr\").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coef dims: \", logreg.coef_.shape)\n",
    "print(\"Intercept dims: \", logreg.intercept_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que está usando 3 clases y dos *features* (lo que tenemos en el *dataset*). Vamos a ver las líneas que define cada uno de los tres conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg.coef_, logreg.intercept_, ['b', 'r', 'g']):\n",
    "    ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "    ax.set_ylim(-10, 15)\n",
    "    ax.set_xlim(-10, 8)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "ax.legend(['Class 0', 'Class 1', 'Class 2', \n",
    "           'Line class 0', 'Line class 1','Line class 2'], \n",
    "           loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada línea muestra la región donde la clase se define como propia o \"resto\".\n",
    "\n",
    "Pero ¿qué pasa con la región del medio que ninguna clase define como propia?\n",
    "\n",
    "Si un punto cae en ese triángulo, ¿a qué clase pertenecerá?\n",
    "\n",
    "Pues será la clase que tenga el valor más alto, es decir, la clase con la línea más próxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_2d_classification(logreg, X, fill=True, alpha=.7, ax=ax)\n",
    "discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg.coef_, logreg.intercept_, ['b', 'r', 'g']):\n",
    "    ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "    ax.legend(['Class 0', 'Class 1', 'Class 2', \n",
    "               'Line class 0', 'Line class 1', 'Line class 2'], \n",
    "               loc=(1.01, 0.3))\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros modelos lineales de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le podéis echar un ojo a `LinearSVC` o a `SGDClassifier` u otros en `sklearn.linear_models`. Del primero comentaremos cosas más adelante. Del segundo comentamos cosas usándolo en regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros apuntes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal parámetro de los modelos lineales es el parámetro de regularización ($\\alpha$ en los modelos de regresión lineales, como la regresión lineal, *Ridge* o Lasso, y `C` en los modelos de clasificación lineales, como la regresión logística, las máquinas de vectores soporte lineales o el clasificador de Gradiente descendiente estocástico. Otra decisión importante en los modelos lineales es el tipo de regularización que queremos usar, principalmente L1 o L2. Dependiendo del problema nos interesará más una estrategia u otra a la hora de seleccionar estos parámetros.\n",
    "\n",
    "Los modelos lineales son muy rápidos de entrenar y en la predicción. Escalan muy bien a conjuntos de datos muy grandes. Otra fortaleza de los modelos lineales es que suele ser más fácil de ver cómo se obtienen las predicciones. Desafortunadamente no siempre es sencillo saber porqué los coeficientes son como son. Esto es especialmente importante si tus datos tienen dimensiones altamente correlacionadas (multicolinealidad) y será complicado interpretar los coeficientes.\n",
    "\n",
    "Los modelos lineales, a menudo, se comportan bien cuando el número de dimensiones es largo comparado con el número de datos. También se usan en casos con gran cantidad de datos, principalmente, porque otros modelos no escalan igual de bien y en muchos casos no es posible usarlos. En casos de baja dimensionalidad otros modelos pueden resultar más interesantes puesto que pueden generalizar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/\n",
    "\n",
    "* Sección 4.4.1 de https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf\n",
    "\n",
    "* https://github.com/martinpella/logistic-reg/blob/master/logistic_reg.ipynb\n",
    "\n",
    "* https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "\n",
    "* https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
