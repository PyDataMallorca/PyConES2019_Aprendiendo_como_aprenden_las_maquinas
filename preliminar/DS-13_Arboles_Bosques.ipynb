{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un árbol de decisión lo que hace es aprender a partir de una jerarquía de preguntas condicionales (\"Si ... si no ...\") llevándonos a poder tomar una decisión. Se pueden usar tanto para clasificación como para regresión. Se dice que es un modo no paramétrico porque no espera que los datos provengan de una distribución en particular, no presupone nada acerca de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior es muy similar a las preguntas que podemos hacer, por ejemplo, en un juego como el **¿Quién es quién?**. Vamos haciendo preguntas cuya respuesta es 'Sí' o 'No' hasta llegar a la solución final. ¿Es mujer?, ¿lleva pendientes?, ¿lleva gafas?, ¿tiene el pelo largo?, ¿lleva gorro?,... Lo que solemos hacer es formular la pregunta que nos elimine más opciones para así tener menos dónde elegir. El algoritmo hace algo parecido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por seguir con el ejemplo del **¿Quién es quién??**. La primera pregunta que formularíamos podría ser, ¿es hombre? o ¿es mujer? ya que en este ejemplo concreto sería el rasgo (*feature*) que mejor nos divide el ejemplo.\n",
    "\n",
    "![arbol quien es quien](./imgs/14_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las respuestas son binarias, 'Sí' o 'No'.\n",
    "\n",
    "![arbol quien es quien](./imgs/15_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si seguimos por la rama del 'Si' vemos que nuestros casos se reducen enormemente. La siguiente pregunta que formularemos sería la que consideremos más interesante para nuestro problema. En este caso uso la pregunta ¿Lleva sombrero? pero podría ser perfectamente ¿lleva el pelo largo? u otras opciones:\n",
    "\n",
    "![arbol quien es quien](./imgs/16_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si seguimos por la rama del 'Sí' nos quedamos ya solo con dos opciones:\n",
    "\n",
    "![arbol quien es quien](./imgs/17_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso ya hemos llegado a las clases 'Claire' o 'Maria'. Si nuestra pregunta es ¿Lleva gafas? y la respuesta es 'Sí' llegariamos a la clase 'Claire'.\n",
    "\n",
    "![arbol quien es quien](./imgs/18_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras preguntas que nos permiten llegar a todas las clases desde la primera rama que hemos usado (Si era hombre o mujer).\n",
    "\n",
    "![arbol quien es quien](./imgs/19_arbol.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rama que va el 'No' de la primera respuesta quizá sería incluso más profunda (por haber más diversidad de datos entre los hombres). Cada cajita con pregunta se conoce como nodo de decisión. El nodo superior se conoce como raíz o *root* y representa todo el conjunto de datos. Se hacen preguntas  y en cada nodo van quedando cada vez menos datos. Un nodo que no se separa más se llama un nodo terminal u hoja. Si el nodo terminal tiene todos los datos de la misma clase se conoce como hoja pura. Las ramas o subárboles son partes del árbol completo. También tenemos nodos padre e hijos donde el nodo padre es de donde surgen las diferentes particiones o nodos hijo, es decir, los nodos hijos son las particiones que se obtienen de un nodo superior o padre.\n",
    "\n",
    "Aprender un árbol de decisión significa aprender la secuencia de preguntas \"Si/Si no\" que nos lleva a la respuesta verdadera de la forma más rápida. Cada una de estas preguntas se conoce como *test* (no confundir con los datos de prueba -> `train-test_split`). En general, los datos van a venir como hemos supuesto más arriba o vendrán representadas como *features* contínuas. En este segundo caso nuestras preguntas serán, más bien, del tipo ¿es la *feature* i mayor que el valor *a*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir el árbol, lo que hace el algoritmo es buscar entre todos los posibles *tests* (preguntas) y encuentra aquel *test*  que resulta más informativo de la variable objetivo. Con cada pregunta estamos particionando los datos dejando particiones cada vez más pequeñas pudiendo refinar las preguntas tanto que al final nos quedemos con un único valor objetivo (una única clase o un único valor de regresión) en cada partición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo decide un árbol dónde se separa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El criterio de decisión sobre cómo se separan los diferentes nodos en un árbol de decisión es diferente en clasificación y en regresión. El árbol separa los nodos usando todas las variables disponibles y luego selecciona la separación que resulta en los subnodos más homogéneos.\n",
    "\n",
    "Por ejemplo, `scikit-learn` puede usar *Gini impurity* o *entropy* para clasificación o variantes del error cuadrático medio o el error absoluto medio para la regresión.\n",
    "\n",
    "Vamos a ver uno de ellos para tener una idea de cómo funciona:\n",
    "\n",
    "Usando *Gini impurity* primero tenemos que calcular su valor para los subnodos usando la fórmula:\n",
    "\n",
    "$$ Gini \\space impurity = 1 - (p^2+q^2) $$\n",
    "\n",
    "donde $ p $ es la probabilidad de éxito y $ q $ es la probabilidad de fracaso (en el caso binario ($ q = 1 - p $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior en detalle aplicado a unos datos será:\n",
    "\n",
    "![gini](./imgs/20_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior tenemos 4 *features* y dos clases, N (Negativo) y P (Positivo). Lo primero que tendríamos que hacer es ver qué *feature* estaría en la parte alta del árbol. Para ello hemos de calcular el *Gini impurity* para cada una de las posibilidades. Empezamos considerando la *feature* \"Tiempo\" y calculamos su *Gini impurity*:\n",
    "\n",
    "![gini](./imgs/21_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior hemos calculado el *Gini impurity* para la primera *feature*. El valor final se obtiene pesando por la probabilidad de cada uno de los casos. Seguimos con el resto de *features*\n",
    "\n",
    "![gini](./imgs/22_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/23_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/24_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/25_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de calcularlo todo (si no nos hemos equivocado con los cálculos) vemos que la *feature* \"Tiempo\" es la menos \"impura\" y es la que nos proporciona  nodos más homogéneos. Incluso obtenemos una hoja (el nodo de en medio solo tiene datos de una clase).\n",
    "\n",
    "Si ahora seguimos con la rama \"Soleado\" tenemos una subtabla (nuestro nodo). En este nodo podemos hacer lo mismo con las tres *features* que quedan.\n",
    "\n",
    "![gini](./imgs/26_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/27_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/28_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/29_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/30_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gini](./imgs/31_arbol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuándo dejariamos de crear nuevos nodos? Si el nodo padre tiene un *Gini impurity* menor que sus nodos hijos quiere decir que estamos generando nodos más impuros y no tiene mucho sentido. Lo que haríamos ahora sería convertir ese nodo padre en una hoja o nodo terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones\n",
    "\n",
    "Existen varias formas de resolver todo esto como ID3, CART, C4.5 o C5.0... Para más información se puede ver https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart. `scikit-learn` usa una forma optimizada de CART (donde CART viene de *Classification And Regression Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo visual con `scikit-learn` de las particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_moons, load_breast_cancer\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "from lib import discrete_scatter\n",
    "from lib.plots import (\n",
    "    plot_tree_progressive, \n",
    "    plot_tree_not_monotone,\n",
    "    plot_tree_partition, \n",
    "    plot_2d_separator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero generamos unos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos los datos que acabamos de crear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_progressive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo más completo con `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Típicamente, construir un árbol y dejar que evolucione hasta que las hojas sean puras no es bueno porque (\\*) <span style=\"color: white\">sobreajusta</span>. Con lo anterior estamos diciendo que en los datos de entrenamiento conseguimos una precisión del 100%. Este es un grave problema de los árboles de decisión. Eso lo podéis ver en el último gráfico anterior.\n",
    "\n",
    "Normalmente lo que se hace es podar el árbol. A esta técnica se la conoce como *pruning* y puede hacerse previamente o posteriormente a obtener el árbol completo. En `scikit-learn` solo hay herramientas para hacer *pre-pruning*. Posibles criterios de pre-poda incluyen el limitar la profundidad máxima del árbol, limitar el número de hojas o requerir un número mínimo de puntos en un nodo para seguir permitiéndole particionarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de sobreajuste usando los datos del cáncer de pecho. Preparamos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, \n",
    "    cancer.target, \n",
    "    stratify=cancer.target, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos nuestro árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y vemos como se está comportando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on training set: {tree.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {tree.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dejado que el árbol evolucione demasiado y lo hemos sobreajustado. El valor del conjunto de pruebas es incluso peor que para algún modelo lineal que hemos visto anteriormente. Para evitar este sobreajuste podemos usar alguna de las cosas que hemos comentado antes. Por ejemplo, vamos a limitar la profundidad del árbol y ver cómo se comporta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(f\"Accuracy on training set: {tree.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {tree.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que obtenemos peor resultado en los datos de entrenamiento pero mejora en los datos de prueba. Estamos generalizando mejor. Si mostramos la instancia del árbol veremos los parámetros que podemos ajustar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cread nuevos árboles y toquetead cosas. Leed la documentación para saber qué estáis tocando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando nuestro árbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representar el árbol nos da mucha capacidad de ver cómo está funcionando el árbol y es algo que puede llegar a entender cualquiera incluso sin necesidad de saber estadística o entender los cálculos subyacentes que se hacen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree, \n",
    "    out_file=\"tree.dot\", \n",
    "    class_names=[\"malignant\", \"benign\"],\n",
    "    feature_names=cancer.feature_names, \n",
    "    impurity=False, \n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El árbol anterior tiene la profundidad limitada y ya está mostrando un montón de ramas y hojas. Si tenemos muchas dimensiones la interpretación puede volverse compleja. Podemos buscar alternativas a mirar el árbol de forma visual y podemos hacer como hicimos anteriormente en los modelos lineales e inspeccionar la importancia de cada *feature*. Esto lo podemos hacer de la siguiente forma en `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature importances:\\n{tree.feature_importances_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo anterior nos da valores entre 0 y 1 para cada *feature* donde 1 indica que tiene mucha importancia y 0 que no tiene ninguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    ax.set_yticks(np.arange(n_features))\n",
    "    ax.set_yticklabels(cancer.feature_names)\n",
    "    ax.set_xlabel(\"Feature importance\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve que la *feature* usada en la primera partición (*worst radius*) es, de lejos, la más importante. Esto confirma nuestra inspección del árbol donde veíamos que en el primer nivel del árbol ya había una buena separación.\n",
    "\n",
    "El hecho de que aquí alguna dimensión tenga poca importancia no significa que no proporcione información. Significa que esa dimensión no ha sido seleccionada por el árbol seguramente debido a que otra dimensión codifica información similar.\n",
    "\n",
    "En contraste a los coeficientes de los modelos lineales, la importancia de las *features* son siempre positivas y no indican qué clase codifica. La importancia de las *features* nos indica que *worst radius* es importante pero no nos dice nada sobre si un radio mayor o menor es indicativo de que un cáncer sea benigno o maligno.\n",
    "\n",
    "Miremos este otro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = plot_tree_not_monotone()\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el anterior ejemplo, la primera *feature*, `X[:,0]` ni siquiera se usa por el árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todo lo anterior hemos estado hablando de clasificación pero para regresión se usa de forma similar pero la clase que tenemos que usar se llama `DecisionTreeRegressor`. Vemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"..\", \"data\", \"ram_price.csv\")\n",
    "\n",
    "ram_prices = pd.read_csv(path)\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "\n",
    "tree =  DecisionTreeRegressor().fit(X_train, y_train)\n",
    "tree3 = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_tree3 = tree3.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_tree3 = np.exp(pred_tree3)\n",
    "price_lr = np.exp(pred_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.semilogy(data_train.date, data_train.price, label=\"Training data\", lw=5)\n",
    "ax.semilogy(data_test.date, data_test.price, label=\"Test data\", lw=5)\n",
    "ax.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\", lw=1, color=\"k\")\n",
    "ax.semilogy(ram_prices.date, price_tree3, label=\"Tree prediction depth=3\", lw=1)\n",
    "ax.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\", lw=1)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué veis en el gráfico anterior? (\\*)\n",
    "\n",
    "<div style=\"color:white\">\n",
    "* El árbol no es capaz de ver más allá del rango donde ha sido entrenado. No parece que vaya a funcionar bien para valores extremos.\n",
    "    \n",
    "* El árbol que no hemos limitado aprende los datos de entrenamiento perfectamente.\n",
    "\n",
    "* Los valores que obtiene son el promedio de lo que encuentra en cada hoja.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y desventajas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas ventajas son:\n",
    "\n",
    "* Son simples de entender y de interpretar ya que se pueden visualizar.\n",
    "\n",
    "* No requieren que hagamos un gran preprocesamiento de los datos.\n",
    "\n",
    "* El coste de usar árboles es logarítmico en los datos usados para entrenarlo.\n",
    "\n",
    "* Pueden manejar datos numéricos y categóricos.\n",
    "\n",
    "* Pueden manejar problemas multi-salida.\n",
    "\n",
    "* Usan una caja blanca. Se puede explicar todo con lógica booleana.\n",
    "\n",
    "* Se puede validar un modelo usando tests estadísticos.\n",
    "\n",
    "* Es útil en la exploración de datos. Nos permiten, de forma rápida, poder ver las variables que están siendo más importantes. \n",
    "\n",
    "Algunas desventajas son:\n",
    "\n",
    "* Pueden crear modelos extremadamente complejos que no generalizan bien. Hay que usar *pruning* y normalmente usaremos validación cruzada para encontrar buenos parámetros para el modelo.\n",
    "\n",
    "* Pueden resultar inestables ya que pequeñas variaciones en los datos pueden llevar a árboles completamente diferentes. Esto se puede mitigar usando *ensembles*.\n",
    "\n",
    "* Los árboles de decisión pueden crear árboles con sesgo si alguna de las clases es dominante. Se recomienda balancear los datos antes de ajustar el modelo.\n",
    "\n",
    "* Hay que andar con ojo con los valores que están en los extremos de la muestra.\n",
    "        \n",
    "Trucos:\n",
    "\n",
    "* Los árboles de decisión suelen sobreajustar en conjuntos de datos con muchas dimensiones. Obtener un buen ratio de número de muestras y dimensiones es importante para evitar el sobreajuste.\n",
    "\n",
    "* Es interesante considerar reducción de la dimensionalidad (PCA, ICA o *Feature selection*) para usar variables que expliquen mejor el problema.\n",
    "\n",
    "* El hecho de entender la estructura del árbol ayudará a la hora de obtener mejor informaciñon de las predicciones que hace el árbol.\n",
    "\n",
    "* Visualiza el árbol empezando con poca profundidad para entender lo que está pasando.\n",
    "\n",
    "* Recuerda que el número de muestras necesario para poblar el árbol se dobla cada vez que añadimos un nuevo nivel.\n",
    "\n",
    "* Usa `min_samples_split` o `min_samples_leaf` para asegurarte que se usan múltiples muestras para informar de cada decisión en el árbol. Un número pequeño de muestras normalmente llevará a sobreajuste. Intenta `min_samples_leaf=5` como valor inicial. Si el número de muestras varía mucho se puede usar un valor decimal como porcentaje para estos dos parámetros. `min_samples_split` puede crear hojas arbitrariamente pequeñas, `min_samples_leaf` garantiza que cada hoja tiene un tamaño mínimo evitando baja varianza, sobreajuste. Para clasificación con pocas clases `min_samples_leaf=1` es a menudo la mejor opción.\n",
    "\n",
    "* Balancea tus datos antes del entrenamiento para evitar sesgos hacia las clases dominantes. Se puede usar `sample_weight`, `min_weight_fraction_leaf`.\n",
    "\n",
    "* Si las muestras están pesadas será más fácil optimizar la estructura del árbol usando `min_weight_fraction_leaf`, que asegura que las hojas contendrán una fracción de la suma total de los pesos de la muestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de árboles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conjuntos (*ensembles*) son métodos que combinan varios modelos de aprendizaje automático para crear modelos más poderosos. Hay muchas formas de crear conjuntos de modelos pero los que han triunfado ampliamente para clasificación y regresión son los bosques aleatorios (*Random forests*) y las *Gradient boosting machines* (máquinas de ¿gradiente aumentado?). Ambos métodos usan árboles de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bosques aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos comentado anteriormente, en la literatura los encontraréis como *Random forests*. Cómo hemos venido viendo, una de las principales debilidades de los árboles de decisión es que tienden a sobreajustar con relativa facilidad. Los bosques aleatorios o *Random Forests* son una forma de aliviar este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *Random forest* es, esencialmente, una colección de árboles de decisión donde cada árbol es ligeramente diferente del resto. La idea principal sería que cada árbol puede hacer un buen trabajo en la predicción pero sobreajustará en parte de los datos. Si plantamos muchos árboles que trabajan bien y sobreajustan de formas diferentes podríamos reducir la cantidad de sobreajuste al promediar sus resultados. Esta reducción del sobreajuste (bajo sesgo, alta varianza) se puede ver matemáticamente pero además seguimos manteniendo la alta capacidad de predicción de los árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar esta estrategia necesitamos plantar muchos árboles de decisión. Cada árbol debería hacer un trabajo digno cuando predice su objetivo y debería ser, también, diferente del resto de árboles. El nombre de *random forests* proviene de introducir aleatoriedad en la construcción de los árboles para asegurarnos que cada árbol es diferente. Hay dos formas de introducir la aleatoriedad, a partir de los puntos usados para construir el árbol y a partir de las *features* usadas en cada partición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar a usar *random forest* hemos de decidir el número de árboles que incluirá (`n_estimators` en `scikit-learn`). Se puede demostrar matemáticamente, bajo ciertas circunstancias, que el número de árboles incluidos se puede relacionar con una disminución de la varianza del orden $ 1/n $ (con $ n $ siendo el número de árboles). Normalmente, a mayor número de árboles el modelo funcionará mejor o similar a un modelo más sencillo. Por ejemplo, queremos construir un bosque aleatorio con 10 árboles. Cada árbol se plantará de forma independiente al resto y el algoritmo realizará elecciones aleatorias para cada árbol para estar seguros que son diferentes entre sí. Para construir el bosque se usa una técnica conocida coloquialmente como *bagging* o *boostrap aggregation*. Lo anterior no es más que coger muestras de tamaño aleatorio y con *features* seleccionadas aleatoriamente con reemplazo. Con reemplazo significa que se pùeden repetir datos en esa muestra. Cada muestra será, en tamaño, igual a la muestra original pero, al haber incluido datos repetidos, algunos de los datos originales no estarán en esta muestra (aproximadamente 1/3 en `scikit-learn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bagging](./imgs/32_bagging.png)\n",
    "\n",
    "[Fuente](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/#seven)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se ha construido el árbol con los nuevos datos pero ahora, en lugar de buscar el mejor *test* para cada nodo el algoritmo selecciona aleatoriamente un subconjunto de *features* en cada nodo y busca el mejor *test* de este subconjunto de *features*. El número máximo de *features* se controla con el parámetro `max_features` en `scikit-learn`. Esta selección aleatoria de *features* se repite en cada nodo de forma separada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos mecanismos (bootstrapping de los datos y selección aleatoria de *features*) aseguran que los árboles construidos sean diferentes. Un parámetro crítico en este proceso es `max_features`. Si elegimos un `max_features` similar a `n_features` significa que cada partición buscará en todas las *features* y no habrá aleatoriedad en la selección de *features*. Si le usamos `max_features=1` entonces solo estaremos ajustando el umbral de la *feature* usada. Por tanto, un valor alto para `max_features` significa que los árboles pueden ser similares y un valor bajo indica que los árboles serán muy diferentes y puede que tengan que ser muy profundos para ajustar bien a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción que obtenemos del bosque se obtiene:\n",
    "\n",
    "* En clasificación: Cada árbol proporciona un voto (selecciona una clase) o una probabilidad para cada clase (*soft-voting*) y el valor ganador (clase seleccionada) es el que más votos tenga o la clase con mayor probabilidad .\n",
    "\n",
    "* En regresión: se usa el promedio de las predicciones de cada árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo usando cinco árboles y el conjunto de datos que vimos anteriormente en un ejemplo anterior de árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles que hemos plantado con el bosque aleatorio se guardan en el atributo `forest.estimators_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora visualizamos las fronteras veremos algo como lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "\n",
    "plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1], alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veis, las particiones que hace cada árbol individual son más 'feas', por decirlo de alguna forma. Además, como no incluyen algunos de los datos (por el *bootstrapping*) desempeñan peor su tarea. Sin embargo, el bosque aleatorio muestra una frontera más suave e intuitiva. Si aumentamos el número de árboles esta frontera tendería a ser más suave. No es raro tener problemas donde se usan cientos o miles de árboles en un bosque aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el ejemplo del cáncer de pecho usando 100 árboles para ver que podemos llegar a ajustar mejor que con los modelos lineales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, \n",
    "    cancer.target, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {forest.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {forest.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior no hemos necesitado ajustar ningún parámetro y, en general, los valores por defecto suelen funcionar aceptablemente bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma que un árbol de decisióm, un bosque aleatorio proporciona información sobre la importancia de las *features* que se obtienen a partir de la importancia de las *features* de cada uno de los árboles individuales. En general, la información que obtenemos del bosque aleatorio sobre la importancia de las *features* es más fiable que la que obtenemos de un único árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, pocas *features* tienen importancia cero. También vemos que el bosque aleatorio le da importancia a `worst radius` pero acaba seleccionando a `worst perimeter` como la *feature* que tiene más importancia. En general, podemos decir que el bosque aleatorio obtiene una mejor vista a partir de los datos que la que se obtiene con un árbol de decisión individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apuntes sobre los *Random forests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bosques aleatorios se encuentran entre los métodos más usados para clasificación y regresión. Son potentes, suelen funcionar bien sin necesidad de ajustar los parámetros y no requieren de escalado de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proporcionan todo lo bueno de los árboles de decisión y resuelven algunos de los problemas de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre los problemas que introduce podríamos decir que se pierde la capacidad de interpretación al no poder analizar en detalle decenas o centenas de árboles en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir bosques grandes requiere de un gran consumo de CPU pero se puede paralelizar de forma sencilla. En `scikit-learn` puedes hacer esto usando el parámetro `n_jobs` para ajustar el número de *cores* a usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se basan en aleatoriedad podemos obtener diferentes resultados usando los mismos datos (excepto si aplicamos un `random_state` para poder reproducir los resultados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que haya más árboles en el bosque el resultado tenderá a ser más robusto y el efecto de la aleatoriedad será menos importante en el resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No suelen funcionar bien con muchas dimensiones o con datos dispersos (por ejemplo, texto). En esos contextos quizá sea mejor usar modelos lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es de esperar, los bosques aleatorios van a necesitar más memoria y CPU que otros modelos que hemos visto. Por tanto, necesitamos más recursos y más tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros más importantes a ajustar son:\n",
    "\n",
    "* `n_estimators`: número de árboles a usar. Números más altos son siempre mejores. Pero requieren más memoria y tiempo por lo que una regla de sentido común suele ser \"entrena el número de árboles que tus recursos y tiempo te permita\".\n",
    "\n",
    "* `max_features`: determina como de aleatorio es cada árbol. Un número más pequeño reduce el sobreajuste. En general, es bueno usar el valor por defecto que diga `scikit-learn` ($ max\\_features = \\sqrt{n\\_features} $ para clasificación y $ max\\_features = \\log_2(n\\_features) $ para regresión). Añadir `max_features` o `max_leaf_nodes` puede, a veces, incrementar el rendimiento. Puede, también, reducir los requerimientos de espacio y tiempo para el entrenamiento y la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Gradient boosting machines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se pueden llamar *gradient boosted regression tree* pero, a pesar del nombre, no solo valen para regresión y se pueden usar también en clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarmente a lo que hacen los bosques aleatorios, también combina múltiples árboles de decisión para crear un modelo superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye los árboles de forma serializada donde cada árbol intenta corregir los errores del árbol previo. Por defecto, no se introduce aleatoriedad y lo que hace es usar una fuerte pre-poda (*pre-pruning*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *gradient boosted trees* suelen usar árboles poco profundos, profundidad de uno a cinco, lo que hace que el modelo sea más pequeño en términos de memoria y velocidad de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principal idea es combinar muchos modelos simples (conocidos como aprendices débiles o *weak learners*), como los árboles poco profundos comentados anteriormente. Cada árbol solo es capaz de proporcionar buenos resultados en parte de los datos y cada vez se van incluyendo más árboles de forma iterativa para mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son los vencedores habituales en muchas competiciones de aprendizaje automático pero son más sensibles al correcto ajuste de sus parámetros que loa árboles de decisión, por lo que pueden llevar más trabajo, aunque con el premio de ofrecer mejores resultados si los parámetros están bien ajustados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además del *pre-pruning* y del número de árboles a usar, otro importante parámetro de estos modelos es la tasa de aprendizaje (`learning_rate` en `scikit-learn`) que controla como de importante es la corrección de los errores que el árbol actual considera sobre los árboles previos. Un valor alto de este parámetro significa que cada árbol hará correcciones más importantes de los errores de los árboles previos permitiendo modelos más complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de añadir más árboles al modelo, vía `n_estimators` incrementa también la complejidad del modelo ya que el modelo tiene más posibilidades de corregir los errores en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo usando `GradientBoostingClassifier` con los datos de cáncer de pecho. Por defecto, si no seleccionamos nada, se usan 100 árboles con una profundidad máxima de 3 y una tasa de aprendizaje de 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, \n",
    "    cancer.target, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos estar sobreajustando. Para reducir ese 100% en los datos de entrenamiento podemos modificar el *pre-pruning* limitando la profundidad máxima o limitar la tasa de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Accuracy on training set: {gbrt.score(X_train, y_train):.3f}\")\n",
    "print(f\"Accuracy on test set: {gbrt.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en otros métodos de árboles de decisión podemos visualizar la importancia de las *features*. Por ejemplo, para un *gradient boosted tree* de 100 árboles con profundidad uno es impráctico inspeccionar los 100 árboles por lo que vemos la importancia conjunta. Nos saldría lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que, más o menos, es similar a lo que hemos visto con los bosques aleatorios aunque vemos también que este modelo ignora completamente algunas de las *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apuntes sobre las *gradient boosting machines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los *gradient boosted trees* funcionan de forma similar a los bosquea aleatorios lo que se suele hacer normalmente es empezar con los segundos. Si estos funcionan bien  pero queremos incluso mejorar la predicción es cuando solemos acudir a los primeros para intentar obtener una mejora del rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes un problema de gran escala puede ser interesante usar el paquete `xgboost` que funciona más rápido y, a veces, es más sencillo para ajustar los parámetros que la implementación de los *gradient boosted trees* incluida en `scikit-learn`. (https://github.com/dmlc/xgboost)\n",
    "\n",
    "También, desde la versión 0.21.0 se puede usar, de forma experimental, `HistGradientBoostingClassifier` y `HistGradientBoostingRegressor`, que son implementaciones inspiradas en `LightGBM`. (https://github.com/Microsoft/LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal problema con los *Gradient boosted regression trees* es que necesitan un ajuste cuidadoso de los parámetros y puede resultar costoso de entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *Gradient boosted regression trees* funcionan bien sin escalar y con mezcla de *features* continuas y binarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como con los bosques aleatorios, no tienen porque funcionar bien cuando hay muchas dimensiones y/o los datos son dispersos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principales parámetros a ajustar con el número de árboles, `n_estimators`, y la tasa de aprendizaje, `learning_rate`. Estos dos parámetros están interconectados. Una `learning_rate` baja implica que necesitaremos más árboles para obtener un modelo de igual complejidad. En contraste con los árboles aleatorios, donde un mayor número de árboles conduce a mejores resultados, en los *Gradient boosted regression trees*, el hecho de incrementar el número de árboles deriva en modelos más complejos que pueden sobreajustar. Una estrategia común es ajustar `n_estimators` basándonos en nuestro presupuesto de tiempo y memoria y, posteriormente, hacer una búsqueda por distintas `learning_rate`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro importante parámetro es `max_depth` (o, como alternativa, `max_leaf_nodes`), para reducir la complejidad de cada árbol. Normalmente, `max_depth` suele tener un valor muy bajo para estos modelos, a menudo menor a cinco particiones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
