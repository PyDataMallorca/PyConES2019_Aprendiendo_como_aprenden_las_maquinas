{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo lineal es uno de los modelos más simples y más ampliamente usados para hacer predicciones. La predicción se obtiene como una combinación lineal de las *features* de los datos ejemplo de entrada.\n",
    "\n",
    "Los modelos lineales se usan principalmente para regresión aunque también se pueden usar para hacer clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo lineal para regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fórmula general para regresión seria algo así:\n",
    "\n",
    "$$ \\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b $$\n",
    "\n",
    "$ x[0] $ a $ x[p] $ son las *features* (en este caso de *p* dimensiones) para un único punto. $w$ y $b$ son los parámetros del modelo que se aprenden a partir de los datos. $\\hat{y}$ es la predicción que hace el modelo.\n",
    "\n",
    "En el caso de una única dimension lo anterior se reduce a:\n",
    "\n",
    "$$ \\hat{y} = w[0] * x[0] + b $$\n",
    "\n",
    "Que os debería de sonar de algo que aplicamos muchas veces en muchos campos. $w[0]$ es la pendiente y $b$ es la ordenada al origen, el *offset* o término independiente.\n",
    "\n",
    "Otra forma de ver lo anterior sería pensar en ello como que $w$ son los pesos para cada *feature* y la predicción no es más que la suma pesada de las *features*.\n",
    "\n",
    "Los modelos lineales para regresión son modelos de regresión para los cuales la predicción viene dada por una línea en datos de una dimensión, un plano cuando tenemos datos de dos dimensiones oa hiperplano para datos de más dimensiones.\n",
    "\n",
    "Existen muchos modelos lineales para regresión. La principal diferencia entre ellos se encuentra en la forma de calcular $w$ y $b$ y en la forma de controlar la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También conocido como ajuste por mínimos cuadrados u *Ordinary Least Squares* (OLS). Es el más simple y más clásico modelo lineal para regresión.\n",
    "\n",
    "La regresión lineal aprende los parámetros $w$ y $b$ minimizando el error cuadrático medio (MSE, por sus siglas en inglés, *Mean Square Error*) obtenido entre las predicciones y los valores objetivo reales.\n",
    "\n",
    "* La regresión lineal no tiene parámetros, lo cual es un beneficio, pero ello lleva a no tener forma de controlar la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo en Python puro sería algo como lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Correlation_and_dependence#Pearson's_product-moment_coefficient\n",
    "# https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.datasets import make_wave\n",
    "\n",
    "class RegresionLineal:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.score_ = None\n",
    "        self.intercept_ = None\n",
    "        self.score2_ = None\n",
    "        self.intercept2_ = None\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        xy = 0\n",
    "        x2 = 0\n",
    "        xm = self._mean(x)\n",
    "        ym = self._mean(y)\n",
    "        m = len(y)\n",
    "        for xi, yi in zip(x, y):\n",
    "            xy += xi * yi\n",
    "            x2 += xi ** 2\n",
    "        self.coef_ = (xy - m * xm * ym) / (x2 - m * xm ** 2)\n",
    "        self.intercept_ = ym - self.coef_ * xm\n",
    "    \n",
    "    def fit2(self, x, y, w0=0, b0=0, alpha=0.001, iters=100):\n",
    "        m = len(y)\n",
    "        for i in range(iters):\n",
    "            b_grad = 0\n",
    "            w_grad = 0\n",
    "            for j in range(m):\n",
    "                b_grad += (2/m) * (w0 * x[j] + b0 - y[j])\n",
    "                w_grad += (2/m) * x[j] * (w0 * x[j] + b0 - y[j])\n",
    "            b0 = b0 - (alpha * b_grad)\n",
    "            w0 = w0 - (alpha * w_grad)\n",
    "        self.score2_ = w0\n",
    "        self.intercept2_ = b0\n",
    "        \n",
    "    def _mean(self, x):\n",
    "        return sum(x) / len(x)\n",
    "    \n",
    "    def _std(self, x):\n",
    "        res = sqrt(sum((xi - self._mean(x))**2 for xi in x) / (len(x) - 1))\n",
    "        return res\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        u = 0\n",
    "        v = 0\n",
    "        ym = self._mean(y)\n",
    "        for xi, yi in zip(x, y):\n",
    "            u += (yi - (self.coef_ * xi + self.intercept_)) ** 2\n",
    "            v += (yi - ym) ** 2\n",
    "        return 1 - u / v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a intentar usarlo a ver lo que obtenemos. Primero obtenemos unos datos con los que trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_wave()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train, y_train, c='k')\n",
    "ax.scatter(X_test, y_test, c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegresionLineal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo entrenamos con los datos de entrenamientos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la pendiente que obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el término independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo ajusta el modelo? ($ R^2 $)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test) #### ¿¿¿¿????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.corrcoef(X_test.T, y_test) ** 2 # ¿¿¿¿????¿¿¿¿????¿¿¿¿????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto lo comentamos en un segundo... ----> Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit2(X_train, y_train, iters=100) # probar valores más altos de iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score2_, model.intercept2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacerlo ahora con `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.datasets import make_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_wave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{linreg.coef_} * x + {linreg.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(linreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [INCISO] Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente descendiente se usa para optimización. Se usa mucho en aprendizaje automático porque es sencillo de entender. No tiene mucho sentido usarlo en un modelo de regresión lineal ya que tiene una solución directa pero se usa en otros algoritmos.\n",
    "\n",
    "Lo que hacemos es básicamente la optimización de una función de pérdida para encontrar nuestros parámetros. En el caso de la regresión lineal lo que se optimiza (minimiza en este caso) es el error y será algo así:\n",
    "\n",
    "$$ f(w, b) = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - (w*x_i+b))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hacemos ahora es obtener el gradiente:\n",
    "\n",
    "$$ \\frac{\\partial{f(w, b)}}{\\partial{w}} = \\frac{-2}{N}\\sum_{i=1}^{N}x_i*(y_i - (w*x_i+b))^2 $$\n",
    "$$ \\frac{\\partial{f(w, b)}}{\\partial{b}} = \\frac{-2}{N}\\sum_{i=1}^{N}(y_i - (w*x_i+b))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros los vamos actualizando de forma iterativa usando una tasa de aprendizaje (*learning rate*):\n",
    "\n",
    "$$ w_{i+1} = w_{i} - (\\alpha * \\frac{\\partial{f(w, b)}}{\\partial{w}}) $$\n",
    "$$ b_{i+1} = b_{i} - (\\alpha * \\frac{\\partial{f(w, b)}}{\\partial{b}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos empezar con unos valores aleatorios para $ w_0 $ y $b_0$. Podemos iterar muchas veces o poner un valor de error donde nuestros parámetros cambian menos que ese valor de error y consideramos que ya es suficientemente correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `scikit-learn`, si queremos hacer una regresión lineal usando *Gradiend descent* podríamos usar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_sgd = SGDRegressor()\n",
    "linreg_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg_sgd.coef_, linreg_sgd.intercept_, linreg_sgd.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SGDRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una clasificación, por ejemplo, binaria funciona de la siguiente forma, la fórmula es muy similar a la que hemos visto para la regresión lineal pero en lugar de devolver la suma pesada de las *features* lo que hacemos es poner un umbral en 0 y si el valor predicho es inferior a 0 entonces estamos prediciendo la clase -1 y si es superior a 0 entonces estamos prediciendo la clase +1. Esta regla de predicción es común para todos los modelos lineales para clasificación. Como en los modelos lineales de regresión tambien encontramos diferentes formas de encontrar las coeficientes $w$ y el término independiente $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(X[:, 0], X[:,1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = SGDRegressor(loss=\"squared_loss\", alpha=0.01, max_iter=200, fit_intercept=True, penalty=None)\n",
    "clf2 = SGDRegressor(loss=\"squared_epsilon_insensitive\", alpha=0.01, max_iter=200, fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.fit(X, y)\n",
    "clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "idx = y == 0\n",
    "ax1.scatter(X[idx,0], X[idx,1], c='b')\n",
    "ax2.scatter(X[idx,0], X[idx,1], c='b')\n",
    "ax1.scatter(X[~idx,0], X[~idx,1], c='g')\n",
    "ax2.scatter(X[~idx,0], X[~idx,1], c='g')\n",
    "\n",
    "for x1 in np.arange(0,4, 0.1):\n",
    "    for x2 in np.arange(0,6, 0.1):\n",
    "        clase = clf1.predict([[x1, x2]])[0]\n",
    "        color = \"b\" if clase <= 0.5 else \"g\"\n",
    "        ax1.scatter(x1, x2, c=color, marker=\"*\", s=50, alpha=0.5)\n",
    "        clase = clf2.predict([[x1, x2]])[0]\n",
    "        color = \"b\" if clase <= 0.5 else \"g\"\n",
    "        ax2.scatter(x1, x2, c=color, marker=\"*\", s=50, alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
