{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "1. **[Modelo de regresión logística](#1.La-regresión-logística)**\n",
    "    * **[1.1 ¿Qué es la regresión logística?](#1.1-¿Qué-es-la-regresión-logística?)**\n",
    "    * **[1.2 Implementación de la regresión logística](#1.2-Implementación-de-la-regresión-logística)**\n",
    "    * **[1.3 Entrenamiento](#1.3-Entrenamiento)**\n",
    "        - **[1.3.1 Dataset de ejemplo](#1.3.1-Dataset-de-ejemplo)**\n",
    "        - **[1.3.2 Visualización del entrenamiento](#1.3.2-Visualización-del-entrenamiento)**\n",
    "2. **[Regresión logística con scikit-learn](#2.-Regresión-logística-con-scikit-learn)**\n",
    "    * **[2.1 Interfaz del modelo](#2.1-Interfaz-del-modelo)**\n",
    "    * **[2.2 Visualización de resultados](#2.2-Visualización-de-resultados)**\n",
    "    \n",
    "3. **[Evaluación del modelo](#3.-Evaluación-del-modelo)**\n",
    "    * **[3.1 Matriz de confusión](#3.1-Matriz-de-confusión)**\n",
    "    * **[3.2 Métricas de clasificación](#3.2-Métricas-de-clasificación)**\n",
    "    \n",
    "4. **[Clasificación multiclase](#4.-Clasificación-multiclase)**\n",
    "    * **[4.1 Fronteras de decisión](#4.1-Fronteras-de-decisión)**\n",
    "    * **[4.2 Evaluación del modelo](#4.2-Evaluación-del-modelo)**\n",
    "    \n",
    "     \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score,\n",
    "                             precision_score, recall_score,\n",
    "                             roc_curve, roc_auc_score, classification_report)\n",
    "\n",
    "from mglearn.datasets import make_forge\n",
    "from mglearn.plots import plot_2d_separator, plot_2d_classification\n",
    "from mglearn import discrete_scatter\n",
    "\n",
    "from code_utils import plot_dataset_2d, plot_model_output, RegLogTrainingPlotter\n",
    "                      \n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.](#Índice)  Modelo de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.1](#Índice) ¿Qué es la regresión logística?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer clasificación lineal podemos usar algoritmos diferentes a los vistos anteriormente. Entre estos algoritmos encontramos la regresión logística. A pesar del nombre, **NO sirve para hacer regresión y solo se usa para clasificación**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear to logistic](./imgs/LR_model_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente se usa para hacer clasificación binaria, aunque se puede extender (la regresión logística y otros algoritmos de clasificación binaria) a clasificación multiclase como luego veremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este caso es similar a lo que vimos anteriormente, tenemos lo siguiente:\n",
    "\n",
    "$$ \\hat{y}(x) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "Salvo que en este caso sumaremos una constante al output de nuestro modelo lineal de la siguiente forma:\n",
    "\n",
    "$$ P(y=1|x) = b + w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "Donde **$x_i$ representa cada una de las features** de nuestro ejemplo, y **$w_i$ representa un vector de pesos**. Por eso, además de encontrar un peso para cada una de las features, nuestro modelo tendrá un parámetro adicional que corresponderá a sumar una constante.\n",
    "\n",
    "Esta constante sumada al output de un modelo lineal se llama **intercept**.\n",
    "\n",
    "\n",
    "\n",
    "Y nuestras clases las podemos definir como:\n",
    "\n",
    "$$ clase_{1} = 1 \\space si \\space P(y=1|x)\\ge 0.5 $$\n",
    "$$ clase_{0} = 0 \\space si \\space P(y=1|x)\\lt 0.5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Las clases da igual como las definamos, pueden ser \"Sí\" y \"No\" o \"No\" y \"Sí\", es decir, simplemente tiene que ser coherente con la pregunta.\n",
    "\n",
    "¿Es azul? Sí -> 0 / No -> 1\n",
    "\n",
    "¿Es azul? Sí -> 1 / No -> 0\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que queremos son probabilidades que nos digan si algo es más probable que esté en una clase o en otra. Para ello se usa la función logística o sigmoide (por su forma en ese):\n",
    "\n",
    "$$ S(x_i) = \\frac{1}{1+e^{-x_i}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear to logistic](./imgs/13_linear_vs_logistic_regression.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2](#Índice) Implementación de la regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con toda esta chicha vamos a construir nuestro algoritmo de regresión logística y evaluar como funciona sobre los datos creados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegLog:\n",
    "    \"\"\"\n",
    "    Modelo de regresión logistica.\n",
    "    \n",
    "    Este modelo solo puede usarse para clasificación binaria (clases {1, 0}). \\\n",
    "     Una vez entrenado, modela la probabilidad de que un ejemplo pertenezca a la clase 1. \n",
    "     \n",
    "    Puede ser usado para predecir la probabilidad de pertenecer a la clase uno, o para\n",
    "     predecir directamente la clase asignada a cada ejemplo.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate: float = 0.001, num_iters: int = 50_000):\n",
    "        \"\"\"\n",
    "        Inicializa una instancia de Reglog. \n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Este valor define cuanto va a actualizarse el model con su \\\n",
    "                           correspondiente gradiente en cada iteración de entrenamiento.\n",
    "            num_iters: Número de iteraciones  que se van a llevar a cabo para \\\n",
    "                        entrenar el modelo.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None # Será un vector de longitud igual al numero de features + 1.\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        string = \"{}\\n\".format(self.__class__.__name__)\n",
    "        if self.weights is None:\n",
    "            string += \"Modelo no entrenado.\"\n",
    "        else:\n",
    "            w_columns = [\"weight_{}: {:.4f} \".format(i, w) for i, w in enumerate(self.weights[1:])]\n",
    "            string += \"\".join(w_columns)\n",
    "            string += \"intercept: {:.4f}\".format(self.weights[0])\n",
    "        return string\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(linear_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Aplica la función sigmoide a cada uno de los valores del array de entrada.\n",
    "\n",
    "        Args:\n",
    "            linear_output: Corresponde a la predicción de un modelo lineal.\n",
    "\n",
    "        Returns:\n",
    "            Aray que contiene el resultado de aplicar la función sigmoide al \\\n",
    "            valor de entrada.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-linear_output))\n",
    "\n",
    "    @staticmethod\n",
    "    def log_likelihood(sigmoid_output, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Función de log-likehood que puede usarse para cuantificar el error del modelo.\n",
    "\n",
    "        Args:\n",
    "            sigmoid_output: Predicción del modelo en forma de probabilidad.\n",
    "            y: Valor real correspondiente a cada predicción realizada por el modelo.\n",
    "\n",
    "        Returns:\n",
    "            Logaritmo de la función de verosimilitud correspondiente a los valores de entrada.\n",
    "        \"\"\"\n",
    "        return (-y * np.log(sigmoid_output) - (1 - y) * np.log(1 - sigmoid_output)).mean()\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Para cada ejemplo, predice la probabilidad de pertenecer a la clase 1.\n",
    "\n",
    "        Args:\n",
    "            X: Dataset que se desea clasificar. Cada ejemplo corresponde a una \\\n",
    "               fila (dimensión 0 del array), y cada feature a una columna \\\n",
    "               (dimensión 1 del array).\n",
    "\n",
    "        Returns:\n",
    "            Array que contiene las probabilidades assignadas a cada ejemplo de pertenecer a la\n",
    "            clase 1. Este vector contiene floats en el intervalo [0, 1].\n",
    "        \"\"\"\n",
    "        # El efecto del intercept es sumar una constante a la predicción lineal del modelo.\n",
    "        # Al aplicar el producto escalar el peso asignado al intercept se multiplica por 1, \n",
    "        # lo cual equivale a sumar el peso, pero es más eficiente de calcular.\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        x = np.concatenate((intercept, X), axis=1)\n",
    "        return self.sigmoid(np.dot(x, self.weights))\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predice la classe a la que pertenecen cada uno de los ejemplos de X.\n",
    "\n",
    "        Args:\n",
    "            X: Dataset que se desea clasificar. Cada ejemplo corresponde a una \\\n",
    "               fila (dimensión 0 del array), y cada feature a una columna \\\n",
    "               (dimensión 1 del array).\n",
    "\n",
    "        Returns:\n",
    "            Devuelve un array unidmiensional que contiene las predicciones para cada ejemplo de X.\n",
    "            Este vector solo puede contener los valores {0, 1}\n",
    "        \"\"\"\n",
    "        return self.predict_proba(X).round()\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Entrena el modelo de regresión logística.\n",
    "\n",
    "        Args:\n",
    "            X_train: Dataset de entrenamiento. Este array contiene \\\n",
    "               las features como columnas y los ejemplos de \\\n",
    "               entrenamiento como filas.\n",
    "\n",
    "            y_train: Classes a las que pertenecen los ejemplos \\\n",
    "               del dataset de entrenamiento codificadas como \\\n",
    "               1 o 0. Cada elemento de este vector corresponde \\\n",
    "               al ejemplo x con el mismo índice. (Un elemento por \\\n",
    "               cada fila de x)\n",
    "        \"\"\"        \n",
    "        self._initialize_weights(X_train)\n",
    "        for i in range(self.num_iters):\n",
    "            self.training_iteration(X_train, y_train)\n",
    "\n",
    "    def training_iteration(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Actualiza los pesos del modelo realizando una iteración del algoritmo gradient descend.\n",
    "        \"\"\"\n",
    "        prediction = self.predict_proba(X_train)\n",
    "        loss = self.log_likelihood(prediction, y_train)\n",
    "        # Guardar estos valores es un truco para calcular el gradiente mas eficientemente.\n",
    "        self._prediction, self._y_train, self._X_train = prediction, y_train, X_train\n",
    "        # En general, el gradiente se calcula derivando la función \n",
    "        # de pérdida usando la regla de la cadena.\n",
    "        loglike_gradient = self._calculate_gradient(loss)\n",
    "        self.weights -= self.learning_rate * loglike_gradient\n",
    "    \n",
    "    def _initialize_weights(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Inicializa los pesos del modelo.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros(n_features + 1) # El + 1 corresponde al peso de intercept.        \n",
    "\n",
    "    def _calculate_gradient(self, loss: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Devuelve el gradiente de la función log likelihood.\n",
    "\n",
    "        Args:\n",
    "            loss: Función de coste que va a derivarse. Será ignorado porque utilizaremos un truco \\\n",
    "                  matemático que no requiere del valor de la función de loss, y que es mas \n",
    "                  eficiente de calcular.\n",
    "\n",
    "        Returns:\n",
    "            Array que contiene el gradiente de loss.\n",
    "        \"\"\"\n",
    "        # Con un poco de matemáticas, podemos darnos cuenta de que el gradiente del \n",
    "        # log-likelihood no es más que la multiplicación de la matriz de entradas transpuesta (X.T)\n",
    "        # por el error de la predicción.\n",
    "        intercept = np.ones((self._X_train.shape[0], 1))\n",
    "        X = np.concatenate((intercept, self._X_train), axis=1)\n",
    "        gradient = np.dot(X.T, (self._prediction - self._y_train)) / self._y_train.size\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.3](#Índice) Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación mostraremos como entrenar nuestro modelo de regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.3.1](#Índice) Dataset de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello utilizaremos un dataset de ejemplo con dos features diferentes y dos clases. Al tener solo dos features, vamos a poder visualizar fácilmente como nuestro modelo classifica los diferentes ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forge, y_forge = make_forge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_forge, y_forge, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_2d(X_train, y_train, X_test, y_test).opts(xlim=(7.5, 12.3), ylim=(-1.5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.3.2](#Índice) Visualización del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestra clase recien creada, y los gráficos en los que visualizaremos como evolucionan la función de pérdida y los pesos del modelo durante el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog = RegLog(num_iters=5500, learning_rate=0.001)\n",
    "reglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot = RegLogTrainingPlotter(reglog, plot_every=150)\n",
    "plot.create_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ajustar el modelo a los datos llamaremos al método `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis modificar la tasa de aprendizaje o el número de iteraciones para ver si mejora o empeora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos con nuestro modelo ajustado los valores de prueba para ver si se parecen a las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reglog.predict(X_test)\n",
    "reglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (yi, yyi, x0, x1) in enumerate(zip(y_pred, y_test, X_test[:,0], X_test[:,1])):\n",
    "    print(\"Example {}, prediction {} true value {} feature_0 {:.3f} feature_1 {:.3f}\".format(i, int(yi), yyi, x0, x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.](#Índice) Regresión logística con scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La libreria scikit-learn ofrece un modelo de regresión logística con más funcionalidades que el mostrado anteriormente, y su uso es muy similar al de la regresión lineal vista anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.1](#Índice) Interfaz del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comparar con lo que trae `scikit-learn`. Instanciamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varias cosas de las que se ven ahí arriba. \n",
    "\n",
    "* El *solver* es lo que realiza la optimización. Podéis leer más sobre ello aquí: https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html. En el caso de scikit-learn usa *solvers* especializados. Más info aquí: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "* `C` es el parámetro que regula la regularización. Un valor bajo regulariza mucho mientras que un valor muy alto le quita importancia a la regularización.\n",
    "\n",
    "* `class_weight` nos permite dar más peso a alguna clase (acordáos de los datos desnivelados o descompensados (*imbalanced datasets*).\n",
    "\n",
    "* `penalty` define el tipo de regularización. Dependiendo del *solver* se podrán usar unos tipos de regularizaciones u otros.\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que el modelo de sklearn se comporte como nuestro modelo anterior tenemos que instanciarlo con los siguientes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e20, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.2](#Índice) Visualización de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación comprobaremos como se comporta el modelo entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (yi, yyi) in enumerate(zip(y_pred, y_test)):\n",
    "    print(\"Example {}, prediction {} true value {}\".format(i, int(yi), yyi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que lo que obtenemos es similar a nuestro algoritmo de más arriba. Vamos a ver los pesos que obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.intercept_, logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_logreg = LogisticRegression().fit(X_train, y_train)\n",
    "plot_model_output(default_logreg, X_train, y_train).opts(title=\"Clasificacion del dataset de entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_output(default_logreg, X_test, y_test).opts(title=\"Clasificacion del dataset de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.](#Índice) Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_utils import (plot_confussion_matrix, plot_decision_boundaries, plot_decision_boundaries,\n",
    "                        interactive_logistic_regression, plot_model_evaluation, plot_feature_importances,\n",
    "                        plot_classification_report\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.1](#Índice) Matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos hablado de la matriz de confusión. Vamos a ver como se ven los modelos usando la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confussion_matrix(y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confussion_matrix(y_test, y_pred, normalize=True, target_names=[\"feature 1\", \"feature 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.2](#Índice) Métricas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una descripción de las métricas, podéis mirar en:\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=[\"feature 1\", \"feature 2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_report(y=y_test, y_pred=y_pred, target_names=[\"feature 1\", \"feature 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.3](#Índice) Pesos del modelo para cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.4](#Índice) Fronteras de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(X_forge, y_forge, logreg.predict(X_forge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4.](#Índice) Clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algoritmos de clasificación que no permiten la clasificación multiclase. La regresión logística es una excepción. Una técnica común para poder usar un algoritmo que solo permite clasificación binaria a un algoritmo con capacidad de clasificación multiclase es usar la estrategia \"Uno contra el Resto\" (*One-Vs-Rest*). La estrategia \"Uno contra el Resto\" no es más que etiquetar la clase que queremos clasificar (por ejemplo con un 1) contra el resto de clases (todas tendrán clase, por ejemplo, 0). De tal forma que tendremos tantos modelos de clasificación binaria como clases queramos clasificar. Para hacer una predicción todos los clasificadores binarios se ejecutan usando un punto de prueba. El clasificador que tiene el *score* más alto en su clase será el \"ganador\" y esa clase se devuelve como el resultado de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver esto con un ejemplo para ver si se entiende mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob, y_blob = make_blobs(random_state=42, n_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dibujar estos datos a ver cómo se ven:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob, ax=ax)\n",
    "ax.set_xlabel(\"Feature 0\")\n",
    "ax.set_ylabel(\"Feature 1\")\n",
    "ax.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "_ = plt.title(\"Dataset con 2 features y 3 target classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos una regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_multi = LogisticRegression(multi_class=\"ovr\").fit(X_blob, y_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coef dims: \", logreg_multi.coef_.shape)\n",
    "print(\"Intercept dims: \", logreg_multi.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_blob = logreg_multi.predict(X_blob)\n",
    "y_pred_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4.1](#Índice) Fronteras de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que está usando 3 clases y dos *features* (lo que tenemos en el *dataset*). Vamos a ver las líneas que define cada uno de los tres conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob, ax=ax)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg_multi.coef_, logreg_multi.intercept_, ['b', 'r', 'g']):\n",
    "    ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "    ax.set_ylim(-10, 15)\n",
    "    ax.set_xlim(-10, 8)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "ax.legend(['Class 0', 'Class 1', 'Class 2', \n",
    "           'Line class 0', 'Line class 1','Line class 2'], \n",
    "           loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada línea muestra la región donde la clase se define como propia o \"resto\".\n",
    "\n",
    "Pero ¿qué pasa con la región del medio que ninguna clase define como propia?\n",
    "\n",
    "Si un punto cae en ese triángulo, ¿a qué clase pertenecerá?\n",
    "\n",
    "Pues será la clase que tenga el valor más alto, es decir, la clase con la línea más próxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_2d_classification(logreg_multi, X_blob, fill=True, alpha=.7, ax=ax)\n",
    "discrete_scatter(X_blob[:, 0], X_blob[:, 1], y_blob, ax=ax)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(logreg_multi.coef_, logreg_multi.intercept_, ['b', 'r', 'g']):\n",
    "    ax.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "    ax.legend(['Class 0', 'Class 1', 'Class 2', \n",
    "               'Line class 0', 'Line class 1', 'Line class 2'], \n",
    "               loc=(1.01, 0.3))\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4.2](#Índice) Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confussion_matrix(y_pred_blob, y_blob, normalize=False, target_names=['Class 0', 'Class 1', 'Class 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros modelos lineales de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le podéis echar un ojo a `LinearSVC` o a `SGDClassifier` u otros en `sklearn.linear_models`. Del primero comentaremos cosas más adelante. Del segundo comentamos cosas usándolo en regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejercicio consiste en analizar como el modelo de regresión logistica es capaz de clasificar el Iris dataset, un dataset de flores que contiene 4 features y 3 especies diferentes de Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a cargar el dataset y a crear un DataFrame que contenga sus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(columns=iris.feature_names, data=iris.data)\n",
    "iris_df[\"species\"] = iris.target\n",
    "iris_df[\"species\"] = iris_df[\"species\"].apply(lambda x: iris.target_names[x])\n",
    "iris_df.columns = [x.replace(\" (cm)\", \"\").replace(\" \", \"_\") for x in iris_df.columns]\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar como las diferentes features se relacionan entre ellas para cada una de las clases con un pair plot. En la diagonal se representan los histogramas de como las clases estan distribuidas, mientras que en las otras celdas se representan scatter plots de todos los pares de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.operation import gridmatrix\n",
    "\n",
    "iris_ds = hv.Dataset(iris_df).groupby('species').overlay()\n",
    "density_grid = gridmatrix(iris_ds, diagonal_type=hv.Distribution, chart_type=hv.Bivariate)\n",
    "point_grid = gridmatrix(iris_ds, chart_type=hv.Points)\n",
    "\n",
    "(density_grid * point_grid).opts(\n",
    "    hv.opts.Bivariate(bandwidth=0.5, cmap=hv.Cycle(values=['Blues', 'Reds', 'Oranges'])),\n",
    "    hv.opts.Points(size=3, alpha=0.5, tools=[\"hover\"]),\n",
    "    hv.opts.NdOverlay(batched=False)).opts(width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio usaremos el dashboard de análisis interactivo para ver como se comporta la regresión logistica con diferentes parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1: Análisis del modelo por defecto\n",
    "\n",
    "- Ejecuta el dashboard con los parámetros por defecto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuántos errores comete el modelo en el dataset de test?\n",
    "2. ¿Cuales son las features más importantes para clasificar cada una de las clases?\n",
    "3. ¿A qué se deben los errores?¿Cómo podrías modificarlo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../../solutions/04_01_default.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2: Estudio sobre regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciona **regularización = \"l1\"** y **C=0.4**. (Si cambias el toggle de \"l1\" a \"none\" podrás comprobar la diferencia sin tener que cambiar C.)\n",
    "\n",
    "4. ¿A qué crees que se debe el cambio en el plot de fronteras de decisión? ¿Qué ventajas e inconvenientes tiene aplicar regularización en este caso?\n",
    "5. ¿Hay alguna opción de regularización que ofrezca mejores resultados que \"l1\"?\n",
    "6. ¿Modificar el parámetro `fit_intercept` tiene algun efecto sobre la regularización?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../../solutions/04_02_regularization.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 3: Modificando el peso de las clases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a comprobar como afecta el parametro class weight al comportamiento de nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Añade a class_weight el siguiente valor: `{0 : 0.333, 1 : 0.333, 2 : 0.333}`. Este diccionario representa el valor por el que los errores en cada clase son ponderados, y cuanto mayor sea el peso de una clase, más se penalizará un error en esta. Se aconseja que su suma sea 1; de lo contrario el proceso de regularización se verá afectado.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ¿Cómo afecta el cambiar los pesos de las diferentes clases a los resultados en el dataset de test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciona regularización \"l1\" y class_weight = `{0: 0.15, 1: 0.53, 2: 0.33}`.\n",
    "\n",
    "8. ¿Crees que el modelo resultante es mejor que los anteriores?¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../../solutions/04_03_weights.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interactive_logistic_regression(iris.data, iris.target,\n",
    "                                target_names=iris.target_names,\n",
    "                                feature_names=iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Apéndice](#Índice) Desarrollo matemático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros apuntes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El principal parámetro de los modelos lineales es el parámetro de regularización ($\\alpha$ en los modelos de regresión lineales, como la regresión lineal, *Ridge* o Lasso, y `C` en los modelos de clasificación lineales, como la regresión logística, las máquinas de vectores soporte lineales o el clasificador de Gradiente descendiente estocástico. Otra decisión importante en los modelos lineales es el tipo de regularización que queremos usar, principalmente L1 o L2. Dependiendo del problema nos interesará más una estrategia u otra a la hora de seleccionar estos parámetros.\n",
    "\n",
    "Los modelos lineales son muy rápidos de entrenar y en la predicción. Escalan muy bien a conjuntos de datos muy grandes. Otra fortaleza de los modelos lineales es que suele ser más fácil de ver cómo se obtienen las predicciones. Desafortunadamente no siempre es sencillo saber porqué los coeficientes son como son. Esto es especialmente importante si tus datos tienen dimensiones altamente correlacionadas (multicolinealidad) y será complicado interpretar los coeficientes.\n",
    "\n",
    "Los modelos lineales, a menudo, se comportan bien cuando el número de dimensiones es largo comparado con el número de datos. También se usan en casos con gran cantidad de datos, principalmente, porque otros modelos no escalan igual de bien y en muchos casos no es posible usarlos. En casos de baja dimensionalidad otros modelos pueden resultar más interesantes puesto que pueden generalizar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A.1](#Índice) Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo llegamos a la función logística?\n",
    "\n",
    "La probabilidad de que un ejemplo $x_i$ sea 1 debido a sus entradas se puede representar de la siguiente forma:\n",
    "\n",
    "$$ P(y=1|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si pensamos que eso se puede representar de forma lineal y tenemos n dimensiones, podríamos escribir algo como:\n",
    "\n",
    "$ P(y=1|x) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $, donde ($ w_0 = b $ y $ x_0 = 1 $), es decir:\n",
    "\n",
    "$$ P(y=1|x) = b + w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "Donde $x_i$ representa cada una de las features de nuestro ejemplo, por lo que además de encontrar un peso para cada una de las features, nuestro modelo tendrá un parámetro adicionar que corresponderá a sumar una constante.\n",
    "\n",
    "Esta constante sumada al output de un modelo lineal se llama intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A.2](#Índice) Derivando la funcion sigmoide a partir del ratio de probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de regresión podría ajustar esos pesos pero, como hemos visto en la gráfica de más arriba eso podría llevar a valores que van de $-\\infty$ a $\\infty$. Dado que queremos predecir probabilidades, que son valores entre 0. y 1. necesitamos encontrar una función capaz de mapear el output de nuestro modelo linea a ese intervalo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ratio de probabilidades (*odds ratio*) es un término que nos puede ayudar. Se define como la probabilidad de que suceda entre la probabilidad de que no suceda. Como nos encontramos en un caso binario, la probabilidad de que suceda es $p$ y la probabilidad de que no suceda es el resto hasta 1, $1-p$. Su ratio se representa así:\n",
    "\n",
    "$$ odds(p) = \\frac{p}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si representamos lo anterior para diferentes valores de $p$ tenemos la siguiente gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01)\n",
    "odds = x / (1 - x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\frac{p}{1-p}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si $p=1$ nuestro ratio se dispara lo cual no es lo que queremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el logaritmo natural del ratio y nos da lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.01, 1, 0.01)\n",
    "odds = np.log(x / (1 - x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\log(\\frac{p}{1-p})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos mapear una combinación lineal de entradas arbitrarias a la función del logaritmo natural de los ratios podemos aprovechar eso para tener:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "($ w_0 = b $ y $ x_0 = 1 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos la $ P(y=1|x) $ podemos buscar la inversa del logaritmo natural ($log\\_odds$) para obtenerlo. Si nos aprovechamos de algunas identidades de logaritmos y exponenciales:\n",
    "\n",
    "$$ y = log(\\frac{x}{1-x}) $$\n",
    "\n",
    "$$ x = log(\\frac{y}{1-y}) $$\n",
    "\n",
    "$$ e^x = \\frac{y}{1-y} $$\n",
    "\n",
    "$$y = (1-y)*e^x $$\n",
    "\n",
    "$$ y = e^x - y*e^x $$\n",
    "\n",
    "$$ y + ye^x = e^x $$\n",
    "\n",
    "$$ y*(1 + e^x) = e^x $$\n",
    "\n",
    "$$ y = \\frac{e^x}{1+e^x} $$\n",
    "\n",
    "$$ y = \\frac{1}{\\frac{1}{e^x} + 1} $$\n",
    "\n",
    "$$ y = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última expresión es la inversa de $log\\_odds$ y se parece mucho a la función logística o sigmoide (en realidad lo es...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.01)\n",
    "inv_odds = 1 / (1 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, inv_odds)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\frac{1}{1+e^{-x}}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, ahora todo se restringe a 0 y 1 en el eje *y*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Explicar que es cada simbolo\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, volviendo al principio teníamos que:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w_0 · x_0 + w_1 · x_1 + ... + w_n · x_n $$\n",
    "\n",
    "($ w_0 = b $ y $ x_0 = 1 $)\n",
    "\n",
    "Si simplificamos la notación tenemos:\n",
    "\n",
    "$$ log\\_odds(P(y=1|x)) = w^T·x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora, finalmente, sacamos la inversa de lo anterior nos quedará:\n",
    "\n",
    "$$ P(y=1|x) = \\frac{1}{1+e^{-w^T·x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores de máxima verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En inglés se conocen como *maximum likelihood estimators* (MLE). Es un método para obtener los parámetros de un modelo estadístico. El método obtiene los parámetros buscando los valores de los parámetros que maximizan la función de verosimilitud. Las estimaciones se conocen como estimadores de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como funciona esto en la práctica. Para ello vamos a usar la distribución normal:\n",
    "\n",
    "$$ f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp(\\frac{-(x-\\mu)^2}{2\\sigma^2})  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de verosimilitud se obtiene de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\mu, \\sigma^2) = f(x_1, x:2,...,x_N|\\mu, \\sigma^2) = \\prod_{i=1}^N f(x_i|\\mu, \\sigma^2) $$\n",
    "$$ L(\\mu, \\sigma^2) = (\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que queremos hacer es maximizar lo anterior. En la expresión anterior tenemos multiplicaciones pero lo que se suele hacer normalmente es usar el logaritmo de la función de verosimilitud. Y en lugar de maximizar se minimiza usando el negativo del logaritmo de la función de verosimilitud:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ll(\\mu, \\sigma^2) = \\log(L(\\mu, \\sigma^2)) $$\n",
    "\n",
    "$$ ll(\\mu, \\sigma^2) = \\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log(L(\\mu, \\sigma^2)) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)} exp(\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "**[INCISO]** Un breve paréntesis para anotar un par de propiedades de exponenciales y de logaritmos naturales:\n",
    "\n",
    "$$ e^x · e ^y = e^{(x + y)} $$\n",
    "\n",
    "$$ ln(x·y) = ln(x) + ln(y) $$\n",
    "\n",
    "$$ ln(x/y) = ln(x) - ln(y) $$\n",
    "\n",
    "$$ ln(x^y) = y·ln(x) $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimos con MLE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desarrollamos la expresión anterior de $nll$ usando las propiedades de exponenciales y logaritmos nos queda:\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\log((\\frac{1}{2\\pi\\sigma^2})^{(n/2)}) - (\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2}) $$\n",
    "\n",
    "$$ nll(\\mu, \\sigma^2) = -\\frac{n}{2}\\log(\\frac{1}{2\\pi\\sigma^2}) - (\\frac{-\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para minimizar hemos de usar el gradiente e igualarlo a 0. Si derivamos con respecto a $\\mu$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\mu}} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\mu}} = \\frac{\\sum_{i=0}^{N}\\mu -\\sum_{i=0}^{N}x_i}{2\\sigma^2}  = 0 $$\n",
    "\n",
    "$$ \\sum_{i=0}^{N}\\mu -\\sum_{i=0}^{N}x_i = 0 $$\n",
    "\n",
    "$$ n·\\mu -\\sum_{i=0}^{N}x_i = 0 $$\n",
    "\n",
    "$$ \\mu = \\frac{\\sum_{i=0}^{N}x_i}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma, derivamos $nll$ con respecto a $\\sigma$ e igualamos a 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\sigma}} = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial{nll(\\mu, \\sigma^2)}}{\\partial{\\sigma}} = \\frac{n}{\\sigma} - \\frac{\\sum_{i=1}^{N}(x_i-\\mu)^2}{\\sigma^3}= 0 $$\n",
    "\n",
    "$$ \\sigma^2 = \\sum_{i=1}^{N}\\frac{(x_i-\\mu)^2}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma obtendríamos los parámetros de la distribución normal.\n",
    "\n",
    "Vamos a aplicar lo mismo ahora para el caso de la regresión logística:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de verosimilitud será (para una clasificación binaria) para un caso (un ejemplo $(x_i, y_i)$) será:\n",
    "\n",
    "$$ L(\\textbf{w})_i = p(x_i|\\textbf{w})^{y_i} · (1-p(x_i|\\textbf{w})^{1-y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo generalizamos para los n casos (datos) tenemos la función de verosimilitud:\n",
    "\n",
    "$$ L(\\textbf{w}) = \\prod_{i=1}^{N}p(x_i|\\textbf{w})^{y_i} · (1-p(x_i|\\textbf{w})^{1-y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *log-likelihood* será:\n",
    "\n",
    "$$ ll(\\textbf{w}) = \\sum_{i=1}^{N}({y_i}·\\log(p(x_i|\\textbf{w})) + {1-y_i}·\\log(1-p(x_i|\\textbf{w}))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si toqueteamos lo anterior llegaremos a lo siguiente (saltando pasos):\n",
    "\n",
    "$$ ll = \\sum_{i=1}^{N}y_{i}\\textbf{w}^{T}x_{i} - log(1+e^{\\textbf{w}^{T}x_{i}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si de lo anterior sacamos el gradiente (saltamos pasos):\n",
    "\n",
    "$$ \\bigtriangledown ll = X^{T}(Y - \\hat{Y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente del *log-likelihood* no es más que la multiplicación de la traspuesta de las entradas por el error de la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/\n",
    "\n",
    "* Sección 4.4.1 de https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf\n",
    "\n",
    "* https://github.com/martinpella/logistic-reg/blob/master/logistic_reg.ipynb\n",
    "\n",
    "* https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "\n",
    "* https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
